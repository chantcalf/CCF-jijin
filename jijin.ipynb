{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trading_date = pd.read_csv('../data/trading_date.csv',names=['day'])\n",
    "trading_date = trading_date[19:619].reset_index(drop=True)\n",
    "trading_date['dd'] = range(len(trading_date))\n",
    "trading_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['fund_pair']\n",
    "for i in range(400):\n",
    "    cols.append(str(i))\n",
    "train_correlation = pd.read_csv('../data/train_correlation.csv')\n",
    "train_correlation.columns = cols\n",
    "for i in range(400):\n",
    "    train_correlation[str(i)] = train_correlation[str(i)].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['fund_pair']\n",
    "for i in range(400,539):\n",
    "    cols.append(str(i))\n",
    "test_correlation = pd.read_csv('../data/test_correlation.csv')\n",
    "test_correlation.columns = cols\n",
    "for i in range(400,539):\n",
    "    test_correlation[str(i)] = test_correlation[str(i)].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261003, 540)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_correlation = train_correlation.merge(test_correlation,on=['fund_pair'],how='left')\n",
    "del test_correlation\n",
    "gc.collect()\n",
    "train_correlation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_pairs = train_correlation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fund_pair = train_correlation[['fund_pair']].copy()\n",
    "fund_pair['fund0'] = fund_pair['fund_pair'].apply(lambda x:int(str(x).split('-')[0].split()[1]))\n",
    "fund_pair['fund1'] = fund_pair['fund_pair'].apply(lambda x:int(str(x).split('-')[1].split()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(723, 601) (723, 601)\n"
     ]
    }
   ],
   "source": [
    "cols = ['fund']\n",
    "for i in range(400):\n",
    "    cols.append(str(i))\n",
    "train_fund_return = pd.read_csv('../data/train_fund_return.csv')\n",
    "train_fund_benchmark_return = pd.read_csv('../data/train_fund_benchmark_return.csv')\n",
    "train_fund_return.columns = cols\n",
    "train_fund_benchmark_return.columns = cols\n",
    "\n",
    "cols = ['fund']\n",
    "for i in range(400,600):\n",
    "    cols.append(str(i))\n",
    "test_fund_return = pd.read_csv('../data/test_fund_return.csv')\n",
    "test_fund_benchmark_return = pd.read_csv('../data/test_fund_benchmark_return.csv')\n",
    "test_fund_return.columns = cols\n",
    "test_fund_benchmark_return.columns = cols\n",
    "\n",
    "train_fund_return = train_fund_return.merge(test_fund_return,on=['fund'],how='left')\n",
    "train_fund_benchmark_return = train_fund_benchmark_return.merge(test_fund_benchmark_return,on=['fund'],how='left')\n",
    "del test_fund_return,test_fund_benchmark_return\n",
    "gc.collect()\n",
    "print (train_fund_return.shape,train_fund_benchmark_return.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = []\n",
    "for i in range(0,600):\n",
    "    cols.append(str(i))\n",
    "train_data2 = train_fund_return[cols].values\n",
    "train_data3 = train_fund_benchmark_return[cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 601)\n"
     ]
    }
   ],
   "source": [
    "cols = ['idx']\n",
    "for i in range(400):\n",
    "    cols.append(str(i))\n",
    "train_index_return = pd.read_csv('../data/train_index_return.csv',encoding='GB2312')\n",
    "train_index_return.columns = cols\n",
    "\n",
    "cols = ['idx']\n",
    "for i in range(400,600):\n",
    "    cols.append(str(i))\n",
    "test_index_return = pd.read_csv('../data/test_index_return.csv',encoding='GB2312')\n",
    "test_index_return.columns = cols\n",
    "\n",
    "train_index_return = train_index_return.merge(test_index_return,on=['idx'],how='left')\n",
    "del test_index_return\n",
    "gc.collect()\n",
    "print (train_index_return.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003286684394535658\n",
      "-0.8916716949699199\n",
      "0.79651343746725\n"
     ]
    }
   ],
   "source": [
    "cols = []\n",
    "for i in range(600):\n",
    "    cols.append(str(i))\n",
    "index_return = train_index_return[cols].values*10\n",
    "print (np.mean(np.mean(index_return)))\n",
    "print (np.min(np.min(index_return)))\n",
    "print (np.max(np.max(index_return)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_len = 100\n",
    "output_len = 61\n",
    "output_len1 = input_len+output_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261003, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ids = fund_pair[['fund0','fund1']].values\n",
    "val_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_X_cols = []\n",
    "for i in range(539-input_len-output_len,539-output_len):\n",
    "    val_X_cols.append(str(i))\n",
    "val_y_cols = []\n",
    "for i in range(539-output_len,539):\n",
    "    val_y_cols.append(str(i))\n",
    "    \n",
    "val_Y = train_correlation[val_y_cols].values.reshape(num_pairs,output_len,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_X = np.zeros((num_pairs,input_len,1))\n",
    "val_X[:,:,0] = train_correlation[val_X_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004608952336021196\n",
      "-0.099565106165259\n",
      "0.45146573480877505\n",
      "0.00030221792565278004\n",
      "-0.11545913369641699\n",
      "0.07343425126393101\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(np.mean(train_data2)))\n",
    "print (np.min(np.min(train_data2)))\n",
    "print (np.max(np.max(train_data2)))\n",
    "\n",
    "print (np.mean(np.mean(train_data3)))\n",
    "print (np.min(np.min(train_data3)))\n",
    "print (np.max(np.max(train_data3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_X2 = np.zeros((num_pairs,output_len1,2))\n",
    "val_X3 = np.zeros((num_pairs,output_len1,2))\n",
    "test_X2 = np.zeros((num_pairs,output_len1,2))\n",
    "test_X3 = np.zeros((num_pairs,output_len1,2))\n",
    "for i in range(num_pairs):\n",
    "    val_X2[i,:,0] = train_data2[val_ids[i,0]-1,539-output_len1:539]\n",
    "    val_X2[i,:,1] = train_data3[val_ids[i,0]-1,539-output_len1:539]\n",
    "    \n",
    "    val_X3[i,:,0] = train_data2[val_ids[i,1]-1,539-output_len1:539]\n",
    "    val_X3[i,:,1] = train_data3[val_ids[i,1]-1,539-output_len1:539]\n",
    "    \n",
    "    test_X2[i,:,0] = train_data2[val_ids[i,0]-1,600-output_len1:600]\n",
    "    test_X2[i,:,1] = train_data3[val_ids[i,0]-1,600-output_len1:600]\n",
    "    \n",
    "    test_X3[i,:,0] = train_data2[val_ids[i,1]-1,600-output_len1:600]\n",
    "    test_X3[i,:,1] = train_data3[val_ids[i,1]-1,600-output_len1:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261003, 161, 35)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx = index_return[:,539-output_len1:539]\n",
    "val_idx = np.swapaxes(val_idx, 0,1)\n",
    "val_idx = np.expand_dims(val_idx,0)\n",
    "val_idx = np.repeat(val_idx,num_pairs,axis=0)\n",
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X_cols = []\n",
    "for i in range(600-output_len-input_len,600-output_len):\n",
    "    test_X_cols.append(str(i))\n",
    "    \n",
    "test_X = np.zeros((num_pairs,input_len,1))\n",
    "test_X[:,:,0] = train_correlation[test_X_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261003, 161, 35)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idx = index_return[:,600-output_len1:600]\n",
    "test_idx = np.swapaxes(test_idx, 0,1)\n",
    "test_idx = np.expand_dims(test_idx,0)\n",
    "test_idx = np.repeat(test_idx,num_pairs,axis=0)\n",
    "test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Layer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(555)\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, data, ids_data,index_return_data,data2,data3,\n",
    "                 batch_size=512, am = 1, alen=100, blen=61, clen=100,shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.am = am\n",
    "        self.data = data\n",
    "        self.data2 = data2\n",
    "        self.data3 = data3\n",
    "        self.index_return_data = index_return_data\n",
    "        self.ids_data = ids_data\n",
    "        self.alen = alen\n",
    "        self.blen = blen\n",
    "        self.clen = clen\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.data) / self.batch_size))*self.am+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        indexes = np.array(range(index*self.batch_size,(index+1)*self.batch_size))\n",
    "        indexes = indexes%len(self.data)\n",
    "        indexes = self.indexes[indexes]\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        X = np.empty((self.batch_size, self.alen, 1))\n",
    "        y = np.empty((self.batch_size, self.blen, 1))\n",
    "        X1 = np.empty((self.batch_size, self.clen,35))\n",
    "        ids = np.empty((self.batch_size, 2))\n",
    "        x2 = np.empty((self.batch_size, self.clen, 2))\n",
    "        x3 = np.empty((self.batch_size, self.clen, 2))\n",
    "\n",
    "        for i in range(len(indexes)):\n",
    "            si = np.random.randint(0,539-self.blen-self.blen-self.alen)\n",
    "            X[i,:,0] = self.data[indexes[i],si:si+self.alen]\n",
    "            \n",
    "            y[i,:,0] = self.data[indexes[i],si+self.alen:si+self.alen+self.blen]\n",
    "            X1[i,:,:] = np.swapaxes(self.index_return_data[:,si+self.alen+self.blen-self.clen:si+self.alen+self.blen],0,1)\n",
    "            ids[i,:] = self.ids_data[indexes[i],:]\n",
    "            x2[i,:,0] = self.data2[self.ids_data[indexes[i],0]-1,si+self.alen+self.blen-self.clen:si+self.alen+self.blen]\n",
    "            x2[i,:,1] = self.data3[self.ids_data[indexes[i],0]-1,si+self.alen+self.blen-self.clen:si+self.alen+self.blen]\n",
    "            \n",
    "            x3[i,:,0] = self.data2[self.ids_data[indexes[i],1]-1,si+self.alen+self.blen-self.clen:si+self.alen+self.blen]\n",
    "            x3[i,:,1] = self.data3[self.ids_data[indexes[i],1]-1,si+self.alen+self.blen-self.clen:si+self.alen+self.blen]\n",
    "\n",
    "        return [ids,X,X1,x2,x3],y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cols = []\n",
    "for i in range(0,539):\n",
    "    train_cols.append(str(i))\n",
    "train_data = train_correlation[train_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 512,\n",
    "          'alen': input_len,\n",
    "          'blen': output_len,\n",
    "          'clen': output_len1,\n",
    "          'am':1,\n",
    "          'shuffle': True}\n",
    "\n",
    "\n",
    "training_generator = DataGenerator(train_data,val_ids,index_return,train_data2,train_data3,  **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7673176\n",
      "-0.43738315\n",
      "0.9999626\n"
     ]
    }
   ],
   "source": [
    "print (np.mean(np.mean(train_data)))\n",
    "print (np.min(np.min(train_data)))\n",
    "print (np.max(np.max(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05956203, 0.95790312])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array([-0.43738315,0.9999626])-0.7673176+1.3)/1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    " \n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    " \n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    " \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    " \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    " \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    " \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    " \n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    " \n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    " \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    " \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    " \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    " \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    " \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    " \n",
    "        a = K.exp(ait)\n",
    " \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    " \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    " \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 2)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 161, 2)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 161, 2)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 100, 1)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2, 16)        11584       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 161, 32)      3360        input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 161, 35)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 100, 1)       0           input_2[0][0]                    \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16)           0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 161, 32)      0           gru_1[0][0]                      \n",
      "                                                                 gru_1[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 161, 64)      19200       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "get_diff_seq_1 (get_diff_seq)   (None, 100, 1)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 61, 16)       0           lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 61, 32)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 61, 64)       0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 99, 1)        0           get_diff_seq_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 61, 112)      0           repeat_vector_3[0][0]            \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     [(None, 99, 256), (N 198144      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     [(None, 61, 256), (N 283392      concatenate_2[0][0]              \n",
      "                                                                 gru_3[0][1]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 61, 256)      0           gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 61, 1)        257         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 61, 1)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "get_add_seq_1 (get_add_seq)     (None, 61, 1)        0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 61, 1)        0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 61, 1)        0           get_add_seq_1[0][0]              \n",
      "                                                                 repeat_vector_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 515,937\n",
      "Trainable params: 515,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class get_diff_seq(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = []\n",
    "        y.append(x[:,0:1,:])\n",
    "        for i in range(1,x.shape[1]):\n",
    "            y.append(x[:,i:i+1,:]-x[:,i-1:i,:])\n",
    "        y = Concatenate(axis=1)(y)\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape)\n",
    "\n",
    "class get_add_seq(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        y = []\n",
    "        y.append(x[:,0:1,:])\n",
    "        for i in range(1,x.shape[1]):\n",
    "            y.append(x[:,i:i+1,:]+y[i-1])\n",
    "        y = Concatenate(axis=1)(y)\n",
    "        return y\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape)\n",
    "\n",
    "def get_basemodel(x_id,x,out_x,out_x1,out_x2):  \n",
    "    #x_id = Lambda(lambda x:K.expand_dims(x,1))(x_id)\n",
    "    temp = Lambda(lambda x:x[:,input_len-1,:input_len])(x)\n",
    "    in_temp = RepeatVector(input_len)(temp)\n",
    "    out_temp = RepeatVector(output_len)(temp)\n",
    "    x = Lambda(lambda x:x[0]-x[1])([x,in_temp])\n",
    "    \n",
    "    x = get_diff_seq()(x)\n",
    "    x = Lambda(lambda x:x[:,1:,:])(x)\n",
    "    \n",
    "    x_id = Embedding(724,16,input_length=2)(x_id)\n",
    "    x_id = Lambda(lambda x:x[:,0,:]*x[:,1,:])(x_id)\n",
    "    #x_id_in = RepeatVector(input_len)(x_id)\n",
    "    x_id_out = RepeatVector(output_len)(x_id)\n",
    "\n",
    "    \n",
    "    hdn1 = 32\n",
    "    rnn1 = GRU(hdn1,return_sequences=True,dropout=0.5,recurrent_dropout=0.5,)\n",
    "    out1_ = rnn1(out_x1)\n",
    "    out2_ = rnn1(out_x2)\n",
    "    diff = Lambda(lambda x: K.abs(x[0] * x[1]))([out1_,out2_])\n",
    "    #in_diff = Lambda(lambda x:x[:,:output_len1-output_len,:])(diff)\n",
    "    out_diff = Lambda(lambda x:x[:,output_len1-output_len:,:])(diff)\n",
    "    #out_diff = RepeatVector(output_len)(diff)\n",
    "    \n",
    "    hdn2 = 64\n",
    "    rnn2 = GRU(hdn2,return_sequences=True,dropout=0.5,recurrent_dropout=0.5, )\n",
    "    out3_ = rnn2(out_x)\n",
    "    #in_idx = Lambda(lambda x:x[:,:output_len1-output_len,:])(out3_)\n",
    "    out_idx = Lambda(lambda x:x[:,output_len1-output_len:,:])(out3_)\n",
    "    #out_idx = RepeatVector(output_len)(out3_)\n",
    "\n",
    "    \n",
    "    hdn3 = 256\n",
    "    rnn3 = GRU(hdn3,return_state=True,return_sequences=True,\n",
    "                  #kernel_initializer='Orthogonal',\n",
    "                  #bias_initializer='zeros', \n",
    "                  #kernel_regularizer=keras.regularizers.l2(0.0001), \n",
    "                  #recurrent_regularizer=keras.regularizers.l2(0.0001),\n",
    "                  dropout=0.5, \n",
    "                  recurrent_dropout=0.5,\n",
    "                 )\n",
    "    de_rnn3 = GRU(hdn3, return_sequences=True, return_state=True,\n",
    "                   #kernel_initializer='Orthogonal',\n",
    "                   #bias_initializer='zeros', \n",
    "                   #kernel_regularizer=keras.regularizers.l2(0.0001), \n",
    "                   #recurrent_regularizer=keras.regularizers.l2(0.0001),\n",
    "                   dropout=0.5, \n",
    "                   recurrent_dropout=0.5,\n",
    "                  )\n",
    "    \n",
    "    #x = Concatenate(axis=2)([x_id_in, x, in_diff, in_idx])\n",
    "    xx = Concatenate(axis=2)([x_id_out, out_diff, out_idx])\n",
    "    #xx = Concatenate(axis=1)([x_id, diff, out3_])\n",
    "    #xx = Dense(128,kernel_initializer='he_normal')(xx)\n",
    "    #xx = BatchNormalization()(xx)\n",
    "    #xx = PReLU()(xx)\n",
    "    #xx = Dropout(0.5)(xx)\n",
    "    #xx = RepeatVector(output_len)(xx)\n",
    "    \n",
    "    out4_,state4 = rnn3(x)\n",
    "    out_x_4,_ = de_rnn3(xx,initial_state=state4)\n",
    "    \n",
    "    \n",
    "    y = out_x_4\n",
    "    #y = Dropout(0.5)(y)\n",
    "    \n",
    "    #y = Dense(128,kernel_initializer='he_normal')(y)\n",
    "    #y = BatchNormalization()(y)\n",
    "    #y = PReLU()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1,kernel_initializer='he_normal',activation='tanh')(y)\n",
    "    y = Lambda(lambda xx:xx*0.1)(y)\n",
    "    y = get_add_seq()(y)\n",
    "    y = add([y,out_temp])\n",
    "    #y = Lambda(lambda xx:xx*1.6-1.3+0.7673176)(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "ids = Input(shape=(2, ))\n",
    "seq_in = Input(shape=(input_len, 1))\n",
    "out_x = Input(shape=(output_len1, 35))\n",
    "out_x1 = Input(shape=(output_len1, 2))\n",
    "out_x2 = Input(shape=(output_len1, 2))\n",
    "seq_out = get_basemodel(ids,seq_in,out_x,out_x1,out_x2)\n",
    "model = Model([ids,seq_in,out_x,out_x1,out_x2],seq_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lc_loss(y_true, y_pred):  \n",
    "    y_true = y_true[:,:,0]\n",
    "    y_pred = y_pred[:,:,0]\n",
    "    MAE = K.mean(K.abs(y_pred - y_true), axis=-1)   \n",
    "    TMAPE = K.mean(K.abs(y_pred-y_true)/(1.5-y_true), axis=-1)  \n",
    "    \n",
    "    y_true1 = y_true[:,50:61]\n",
    "    y_pred1 = y_pred[:,50:61]\n",
    "    MAE1 = K.mean(K.abs(y_pred1 - y_true1), axis=-1)   \n",
    "    TMAPE1 = K.mean(K.abs(y_pred1-y_true1)/(1.5-y_true1), axis=-1) \n",
    "    \n",
    "    return MAE + TMAPE + (MAE1 + TMAPE1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lc_metric(y_true, y_pred):\n",
    "    y_true = y_true[:,:,0]\n",
    "    y_pred = y_pred[:,:,0]\n",
    "    MAE = K.mean(K.abs(y_pred-y_true), axis=-1)   \n",
    "    TMAPE = K.mean(K.abs(y_pred-y_true)/(1.5-y_true), axis=-1)  \n",
    "    return (2/(2+MAE+TMAPE))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lc_metric1(y_true, y_pred):\n",
    "    y_true = y_true[:,50:61,0]\n",
    "    y_pred = y_pred[:,50:61,0]\n",
    "    MAE = K.mean(K.abs(y_pred-y_true), axis=-1)   \n",
    "    TMAPE = K.mean(K.abs(y_pred-y_true)/(1.5-y_true), axis=-1)  \n",
    "    return (2/(2+MAE+TMAPE))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 512*2,\n",
    "          'alen': input_len,\n",
    "          'blen': output_len,\n",
    "          'clen': output_len1,\n",
    "          'am':1,\n",
    "          'shuffle': True}\n",
    "\n",
    "training_generator = DataGenerator(train_data,val_ids,index_return,train_data2,train_data3,  **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = Input(shape=(2, ))\n",
    "seq_in = Input(shape=(input_len, 1))\n",
    "out_x = Input(shape=(output_len1, 35))\n",
    "out_x1 = Input(shape=(output_len1, 2))\n",
    "out_x2 = Input(shape=(output_len1, 2))\n",
    "seq_out = get_basemodel(ids,seq_in,out_x,out_x1,out_x2)\n",
    "model = Model([ids,seq_in,out_x,out_x1,out_x2],seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "255/255 [==============================] - 286s 1s/step - loss: 0.3352 - lc_metric: 0.8812 - lc_metric1: 0.8397 - val_loss: 0.4689 - val_lc_metric: 0.8583 - val_lc_metric1: 0.7752\n",
      "\n",
      "Epoch 00001: val_lc_metric improved from -inf to 0.85832, saving model to ../data/keras.model\n",
      "Epoch 2/100\n",
      "255/255 [==============================] - 282s 1s/step - loss: 0.3301 - lc_metric: 0.8828 - lc_metric1: 0.8419 - val_loss: 0.4469 - val_lc_metric: 0.8653 - val_lc_metric1: 0.7833\n",
      "\n",
      "Epoch 00002: val_lc_metric improved from 0.85832 to 0.86529, saving model to ../data/keras.model\n",
      "Epoch 3/100\n",
      "255/255 [==============================] - 281s 1s/step - loss: 0.3279 - lc_metric: 0.8836 - lc_metric1: 0.8429 - val_loss: 0.4453 - val_lc_metric: 0.8651 - val_lc_metric1: 0.7844\n",
      "\n",
      "Epoch 00003: val_lc_metric did not improve from 0.86529\n",
      "Epoch 4/100\n",
      "255/255 [==============================] - 281s 1s/step - loss: 0.3268 - lc_metric: 0.8839 - lc_metric1: 0.8434 - val_loss: 0.4438 - val_lc_metric: 0.8656 - val_lc_metric1: 0.7849\n",
      "\n",
      "Epoch 00004: val_lc_metric improved from 0.86529 to 0.86564, saving model to ../data/keras.model\n",
      "Epoch 5/100\n",
      "255/255 [==============================] - 281s 1s/step - loss: 0.3262 - lc_metric: 0.8840 - lc_metric1: 0.8436 - val_loss: 0.4245 - val_lc_metric: 0.8704 - val_lc_metric1: 0.7931\n",
      "\n",
      "Epoch 00005: val_lc_metric improved from 0.86564 to 0.87039, saving model to ../data/keras.model\n",
      "Epoch 6/100\n",
      "255/255 [==============================] - 283s 1s/step - loss: 0.3243 - lc_metric: 0.8844 - lc_metric1: 0.8445 - val_loss: 0.4259 - val_lc_metric: 0.8700 - val_lc_metric1: 0.7923\n",
      "\n",
      "Epoch 00006: val_lc_metric did not improve from 0.87039\n",
      "Epoch 7/100\n",
      "255/255 [==============================] - 282s 1s/step - loss: 0.3239 - lc_metric: 0.8845 - lc_metric1: 0.8446 - val_loss: 0.4538 - val_lc_metric: 0.8622 - val_lc_metric1: 0.7809\n",
      "\n",
      "Epoch 00007: val_lc_metric did not improve from 0.87039\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 8/100\n",
      "255/255 [==============================] - 282s 1s/step - loss: 0.3231 - lc_metric: 0.8847 - lc_metric1: 0.8450 - val_loss: 0.4370 - val_lc_metric: 0.8671 - val_lc_metric1: 0.7875\n",
      "\n",
      "Epoch 00008: val_lc_metric did not improve from 0.87039\n",
      "Epoch 9/100\n",
      "255/255 [==============================] - 282s 1s/step - loss: 0.3229 - lc_metric: 0.8847 - lc_metric1: 0.8452 - val_loss: 0.4388 - val_lc_metric: 0.8667 - val_lc_metric1: 0.7867\n",
      "\n",
      "Epoch 00009: val_lc_metric did not improve from 0.87039\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "Epoch 10/100\n",
      "255/255 [==============================] - 283s 1s/step - loss: 0.3235 - lc_metric: 0.8846 - lc_metric1: 0.8450 - val_loss: 0.4359 - val_lc_metric: 0.8673 - val_lc_metric1: 0.7880\n",
      "\n",
      "Epoch 00010: val_lc_metric did not improve from 0.87039\n",
      "Epoch 11/100\n",
      "255/255 [==============================] - 284s 1s/step - loss: 0.3228 - lc_metric: 0.8847 - lc_metric1: 0.8454 - val_loss: 0.4359 - val_lc_metric: 0.8672 - val_lc_metric1: 0.7880\n",
      "\n",
      "Epoch 00011: val_lc_metric did not improve from 0.87039\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26ddceddd30>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(patience=6,\n",
    "                               verbose=1,\n",
    "                               monitor='val_lc_metric',\n",
    "                               mode='max',\n",
    "                              )\n",
    "model_checkpoint = ModelCheckpoint(\"../data/keras.model\",\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=1,\n",
    "                                   monitor='val_lc_metric',\n",
    "                                   mode='max',\n",
    "                                  )\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.1, \n",
    "                              patience=2, \n",
    "                              min_lr=0.00001, \n",
    "                              verbose=1,\n",
    "                              monitor='val_lc_metric',\n",
    "                              mode='max',\n",
    "                             )\n",
    "\n",
    "model.compile(\n",
    "              #loss='mse', \n",
    "              loss=lc_loss,\n",
    "              optimizer=keras.optimizers.Nadam(lr=0.0002),\n",
    "              metrics=[lc_metric,lc_metric1],\n",
    "             )\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=[[val_ids,val_X,val_idx,val_X2,val_X3],val_Y],\n",
    "                    epochs=epochs,\n",
    "                    #batch_size=batch_size,\n",
    "                    callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "                    #use_multiprocessing=False,\n",
    "                    #workers=1,\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.87544 0.7903\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.85859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"../data/keras.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict([val_ids,test_X,test_idx, test_X2,test_X3],batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261003, 61, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26ddd0bee48>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD0CAYAAACLpN0/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ii = 11000\n",
    "plt.plot(range(439,539),train_data[ii,439:],color='blue')\n",
    "plt.plot(range(539,539+61),y_pred[ii,:],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fund_pair', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '529', '530', '531', '532', '533', '534', '535', '536', '537', '538'],\n",
       "      dtype='object', length=540)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_correlation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['ID'] = train_correlation['fund_pair']\n",
    "#sub['value'] = np.mean(y_pred[:,50:61,0],axis=1)\n",
    "uuu = ['538','111','222','333','444','66','88','528','518']\n",
    "#uuu = ['66','88','528','518']\n",
    "sub['value'] = np.mean(np.array(train_correlation[uuu].values),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('../submit/1111_random.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    261003.000000\n",
       "mean          0.831386\n",
       "std           0.085967\n",
       "min           0.106672\n",
       "25%           0.792284\n",
       "50%           0.850266\n",
       "75%           0.891611\n",
       "max           0.999556\n",
       "Name: value, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
